#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\begin_preamble
\usepackage[left,modulo]{lineno}
\usepackage{amsmath}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Bayesian Regression
\end_layout

\begin_layout Author
Assaf Dvora
\end_layout

\begin_layout Date
June 2019
\end_layout

\begin_layout Section
The Linear Regression Problem 
\end_layout

\begin_layout Standard
Assume we have a training set 
\begin_inset Formula $\mathcal{{D}}$
\end_inset

 of 
\begin_inset Formula $n$
\end_inset

 observations, 
\begin_inset Formula $\mathcal{{D}}=\{(\boldsymbol{x}_{i},y_{i})|i=1,..,n\}$
\end_inset

, where 
\begin_inset Formula $\boldsymbol{x}\in\mathbb{{R}}^{d}$
\end_inset

 denotes an input vector (covariates) of dimension 
\begin_inset Formula $\mathcal{{D}}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 denotes a scalar output or target (dependent variable).
 The columns vector inputs for all 
\begin_inset Formula $n$
\end_inset

 cases are aggregated in the 
\begin_inset Formula $D\times n$
\end_inset

 design matrix 
\begin_inset Formula $X\in\mathbb{{R}}^{D\times n}$
\end_inset

 and the targets are collected in the vector 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

.
 We can write 
\begin_inset Formula $\mathcal{{D}=}(X,\boldsymbol{y})$
\end_inset

.
 We are interested in making inferences about the relationship between inputs
 and targets, i.e.
 the conditional distribution of the targets given the inputs (but we are
 not interested in modeling the input distribution itself.
 
\end_layout

\begin_layout Section
Bayesian Linear Regression 
\end_layout

\begin_layout Standard
Linear regression assumes that there is a linear relationship between inputs
 and outputs, up to some noise term 
\begin_inset Formula $\epsilon$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y=\boldsymbol{x}^{T}\boldsymbol{w}+\epsilon,
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 is a vector of weights (parameters) of the linear model.
 Often bias term is added, but as this can be implemented by augmenting
 the input vector 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 with additional element whose value is always one, we do not explicitly
 include it in our notation.
 We further assume that this noise term follows an IID Gaussian distribution
 with zero mean and variance 
\begin_inset Formula $\sigma_{\epsilon}^{2}$
\end_inset


\begin_inset Formula 
\begin{equation}
\epsilon\sim\mathcal{{N}}(0,\sigma_{\epsilon}^{2}).
\end{equation}

\end_inset

 The likelihood results in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
p(\boldsymbol{y}|X,\boldsymbol{w}) & =\prod_{i=1}^{n}p(y_{i}|\boldsymbol{x}_{i},\boldsymbol{w})=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma_{\epsilon}^{2}}}\exp\left[-\frac{(y_{i}-\boldsymbol{x}_{i}^{T}\boldsymbol{w})^{2}}{2\sigma_{\epsilon}^{2}}\right]=\nonumber \\
 & =\frac{1}{(2\pi\sigma_{\epsilon}^{2})^{n/2}}\exp\left[-\frac{1}{2\sigma_{\epsilon}^{2}}\sum_{i=1}^{n}(y_{i}-\boldsymbol{x}_{i}^{T}\boldsymbol{w}_{i})^{2}\right]=\mathcal{{N}}(X^{T}\boldsymbol{w},\sigma_{\epsilon}^{2}I).
\end{align}

\end_inset

In the Bayesian formalism we need to specify a 
\shape italic
prior 
\shape default
over the parameters.
 We put a zero mean Gaussian prior with covariance matrix 
\begin_inset Formula $\Sigma_{p}$
\end_inset

 on the weights
\begin_inset Formula 
\begin{equation}
\boldsymbol{w}\sim\mathcal{{N}}(0,\Sigma_{p}).
\end{equation}

\end_inset

The posterior distribution over the weights is given by,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(\boldsymbol{w}|\boldsymbol{y},X)=\frac{p(\boldsymbol{y}|X,\boldsymbol{w})p(\boldsymbol{w})}{p(\boldsymbol{y}|X)}=\frac{p(\boldsymbol{y}|X,\boldsymbol{w})p(\boldsymbol{w})}{\int{p(\boldsymbol{y}|X,\boldsymbol{w})p(\boldsymbol{w})d\boldsymbol{w}}}.
\end{equation}

\end_inset

Since the prior and the likelihood are conjugate Gaussian distributions,
 the posterior is Gaussian:
\begin_inset Formula 
\begin{equation}
p(\boldsymbol{w}|\boldsymbol{y},X)=\mathcal{{N}}(\boldsymbol{\mu}_{w},\Sigma_{w}),
\end{equation}

\end_inset

and 
\begin_inset Quotes eld
\end_inset

completing the square
\begin_inset Quotes erd
\end_inset

 we obtain, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mu_{w}=\frac{1}{\sigma_{\epsilon}^{2}}A^{-1}X\boldsymbol{y},\quad\Sigma_{w}=A^{-1},
\end{equation}

\end_inset

where 
\begin_inset Formula $A=\sigma_{\epsilon}^{-2}XX^{T}+\Sigma_{p}^{-1}.$
\end_inset

 We note that for Gaussian posterior the mean vector is also the MAP estimate
 of 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

.
\end_layout

\begin_layout Standard
To make predictions for a test case we average over all possible parameter
 values, weighted by their posterior probability.
 Thus, the predictive probability is given by,
\begin_inset Formula 
\begin{equation}
p(y_{*}|\boldsymbol{x}_{*},X,\boldsymbol{y})=\int{p(y_{*}|\boldsymbol{x}_{*},\boldsymbol{w})p(\boldsymbol{w}|\boldsymbol{y},X)d\boldsymbol{w}}
\end{equation}

\end_inset

in the case of linear model with Gaussian posterior, the predictive probability
 is also Gaussian, 
\begin_inset Formula 
\begin{equation}
p(y_{*}|\boldsymbol{x}_{*},X,\boldsymbol{y})=\mathcal{{N}}(\frac{1}{\sigma_{\epsilon}^{2}}\boldsymbol{x}_{*}^{T}A^{-1}X\boldsymbol{y},\:\boldsymbol{x}_{*}^{T}A^{-1}\boldsymbol{x}_{*}).\label{eq:pred-prob}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Bayesian Linear Regression Using Gaussian Process.
 
\end_layout

\begin_layout Standard
Lets assume the following additive model for the response variable
\begin_inset Formula 
\begin{equation}
y=f(\boldsymbol{x})+\varepsilon,
\end{equation}

\end_inset

where 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 is unobserved latent 
\series bold
stochastic process
\series default
 indexed by the input vector 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

, and 
\begin_inset Formula $\varepsilon$
\end_inset

 is i.i.d Gaussian noise with variance 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

.
 Hence, the response variable satisfies
\begin_inset Formula 
\begin{equation}
p(y|f(\boldsymbol{x}),\sigma_{\varepsilon}^{2})=\mathcal{{N}}(f(\boldsymbol{x}),\sigma_{\varepsilon}^{2}).
\end{equation}

\end_inset

We further assume that 
\begin_inset Formula $\sigma_{\varepsilon}^{2}$
\end_inset

 is known and hence we use 
\begin_inset Formula $p(y|f(\boldsymbol{x}))$
\end_inset

 instead.
 
\end_layout

\begin_layout Standard
Denote with 
\begin_inset Formula $\boldsymbol{f}=[f(\boldsymbol{x}_{1}),f(\boldsymbol{x}_{2}),...,f(\boldsymbol{x}_{n})]^{T}\in R^{n}$
\end_inset

 the vector of realizations of the random process.
 The likelihood of the model is given by 
\begin_inset Formula 
\begin{equation}
p(\mathcal{{D}}|\boldsymbol{f})=p(\boldsymbol{y}|\boldsymbol{f})=\prod_{i=1}^{n}p(y_{i}|f(x_{i}),\sigma_{\varepsilon}^{2})=\mathcal{{N}}(\boldsymbol{f},\sigma_{\varepsilon}^{2}I).\label{eq:likelihood}
\end{equation}

\end_inset

To define a prior over 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 we use the Bayesian Linear Regression model, 
\begin_inset Formula $f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}$
\end_inset

, with prior 
\begin_inset Formula $\boldsymbol{w}\sim\mathcal{{N}}(0,\Sigma_{w})$
\end_inset

.
 We show in Appendix A that this results in a Gaussian process for 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 with the mean and covariance satisfying
\begin_inset Formula 
\begin{align}
E[f(\boldsymbol{x})] & =0,\nonumber \\
E[f(\boldsymbol{x})f(\boldsymbol{x}')] & \overset{\Delta}{{=}}\mathcal{{K}}(\boldsymbol{x},\boldsymbol{x}')=\boldsymbol{x}^{T}\Sigma_{w}\boldsymbol{x}'.\label{eq:cov func f}
\end{align}

\end_inset

Thus, the prior probability of the latent vector 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{equation}
\boldsymbol{f\sim}\mathcal{{N}}(\boldsymbol{0},K),\label{eq:prior f}
\end{equation}

\end_inset

 where 
\begin_inset Formula $K=X^{T}\Sigma_{w}X$
\end_inset

.
 Based on Bayes theorem, the posterior probability can be written as 
\begin_inset Formula 
\begin{equation}
p(\boldsymbol{f}|\mathcal{{D}})=\frac{p(\mathcal{{D}}|\boldsymbol{f})p(\boldsymbol{f})}{p(\mathcal{{D}})}\label{eq:posterior}
\end{equation}

\end_inset

since the prior probability 
\begin_inset Formula $p(\boldsymbol{f})$
\end_inset

 and the likelihood function 
\begin_inset Formula $p(\mathcal{{D}}|\boldsymbol{f})$
\end_inset

 are conjugate Gaussian distributions, the posterior is Gaussian and satisifies
\begin_inset Formula 
\begin{equation}
p(\boldsymbol{f}|\mathcal{{D}})=\mathcal{{N}}(\boldsymbol{\mu}_{p},\boldsymbol{\Sigma}_{p}).
\end{equation}

\end_inset

We show in Appendix B that the mean vector, 
\begin_inset Formula $\boldsymbol{\mu}_{p}$
\end_inset

, and the covaraince matrix, 
\begin_inset Formula $\Sigma_{p}$
\end_inset

 are given by
\begin_inset Formula 
\begin{align}
\boldsymbol{\mu}_{p} & =K(\sigma_{\varepsilon}^{2}I+K)^{-1}\boldsymbol{y}\label{eq:mean posterior}\\
\Sigma_{p} & =\sigma_{\varepsilon}^{2}(\sigma_{\varepsilon}^{2}I+K)^{-1}K\label{eq:covaraince posterior}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For a test case, 
\begin_inset Formula $\boldsymbol{x}_{*}$
\end_inset

, we would like to predict the latent varaibale 
\begin_inset Formula $f_{*}\overset{\Delta}{{=}}f(\boldsymbol{x}_{*})$
\end_inset

 given the knowledge we have about our data.
 The posterior predictive distibution 
\begin_inset Formula $p(f_{*}|\boldsymbol{x}_{*}\mathcal{{D}})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(f_{*}|\mathcal{{D}})=\int p(f_{*}|\boldsymbol{f})p(\boldsymbol{f}|\mathcal{{D}})d\boldsymbol{f}\label{eq:posterior predictive}
\end{equation}

\end_inset

where 
\begin_inset Formula $p(f_{*}|\boldsymbol{f})$
\end_inset

 is the (prior) conditional distibution of 
\begin_inset Formula $f_{*}$
\end_inset

 given 
\series bold

\begin_inset Formula $\boldsymbol{f}$
\end_inset


\series default
 and 
\begin_inset Formula $p(\boldsymbol{f}|\mathcal{{D}})$
\end_inset

 is the posterior distrbution of 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

.
 We note that 
\begin_inset Formula $\boldsymbol{f}$
\end_inset

 and 
\begin_inset Formula $f_{*}$
\end_inset

 are jointly Gaussian
\begin_inset Formula 
\begin{equation}
\left[\begin{array}{c}
\begin{array}{c}
\boldsymbol{f}\\
f_{*}
\end{array}\end{array}\right]\sim\mathcal{{N}}\left[\left(\begin{array}{c}
\begin{array}{c}
\boldsymbol{0}\\
0
\end{array}\end{array}\right),\left(\begin{array}{c}
\begin{array}{cc}
K & \boldsymbol{k_{*}}\\
\boldsymbol{k}_{*}^{T} & \mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})
\end{array}\end{array}\right)\right]
\end{equation}

\end_inset

where 
\begin_inset Formula $\boldsymbol{k}_{*}=[\mathcal{{K}}(\boldsymbol{x}_{1},\boldsymbol{x}_{*}),\mathcal{{K}}(\boldsymbol{x}_{2},\boldsymbol{x}_{*}),...,\mathcal{{K}}(\boldsymbol{x}_{n},\boldsymbol{x}_{*})]$
\end_inset

.
 The conditional distribution 
\begin_inset Formula $p(f_{*}|\boldsymbol{f})$
\end_inset

 is also Gaussian and is given by (Rasmussen et al.) 
\begin_inset Formula 
\begin{equation}
p(f_{*}|\boldsymbol{f})=\mathcal{N}(\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{f},\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{k}_{*}).
\end{equation}

\end_inset

In Appendix C we show that the posterior predictive distribution, 
\begin_inset Formula $p(f_{*}|\mathcal{{D}})$
\end_inset

 is Gaussian and satisfies 
\begin_inset Formula 
\begin{align}
\mathcal{\mathbb{{E}}}[f_{*}|\mathcal{{D}}] & =\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{\mu}_{p}\label{eq:post-pred-mean}\\
\mathrm{var}[f_{*}|{D}] & =\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}(K^{-1}-K^{-1}\Sigma_{p}K^{-1})\boldsymbol{k}_{*}\label{eq:post-pred-var}
\end{align}

\end_inset

where 
\begin_inset Formula $\boldsymbol{\mu}_{p}$
\end_inset

 and 
\begin_inset Formula $\Sigma_{p}$
\end_inset

 are the mean vector and covariance matrix of the posterior 
\begin_inset Formula $p(\boldsymbol{f}|\mathcal{{D}})$
\end_inset

 (respectively).
 Substituitng (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mean posterior"
plural "false"
caps "false"
noprefix "false"

\end_inset

) to (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:post-pred-mean"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we obtain the mean value of the posterior predictive distribution
\begin_inset Formula 
\begin{equation}
\mathcal{\mathbb{{E}}}[f_{*}|\mathcal{{D}}]=\boldsymbol{k}_{*}^{T}(K+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{y}.\label{eq:post-pred-mean-1}
\end{equation}

\end_inset

To obtain an expression for the posterior predictive variance we first note
 that the posterior covariance matrix, 
\begin_inset Formula $\Sigma_{p}$
\end_inset

 can be rearranged to
\begin_inset Formula 
\begin{align}
\Sigma_{p} & =\sigma_{\varepsilon}^{2}(\sigma_{\varepsilon}^{2}I+K)^{-1}K=(I+\sigma_{\varepsilon}^{-2}K)^{-1}K\nonumber \\
 & =\left[K^{-1}(I+\sigma_{\varepsilon}^{-2}K)\right]^{-1}=(K^{-1}+\sigma_{\varepsilon}^{-2}I)^{-1}.\label{eq:cov_posterior_arr}
\end{align}

\end_inset

Substituting (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cov_posterior_arr"
plural "false"
caps "false"
noprefix "false"

\end_inset

) to (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:post-pred-var"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we obtain,
\begin_inset Formula 
\begin{equation}
\mathrm{var}[f_{*}|{D}]=\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}(K^{-1}-K^{-1}(K^{-1}+\sigma_{\varepsilon}^{-2}I)^{-1}K^{-1})\boldsymbol{k}_{*}
\end{equation}

\end_inset

Using the matrix inversion lemma (Rasmussen et al., Appendix A), we obtain
\begin_inset Formula 
\begin{equation}
\mathrm{var}[f_{*}|{D}]=\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}(K+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{k}_{*}.\label{eq:post-pred-var-1}
\end{equation}

\end_inset

Thus, the posterior predictive distribution satisfies
\begin_inset Formula 
\begin{equation}
p(\boldsymbol{f}|\mathcal{{D}})=\mathcal{{N}}(\boldsymbol{k}_{*}^{T}(K+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{y},\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}(K+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{k}_{*})\label{eq:post-pred-dist}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Next we show that the posterior predictive distribution (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:post-pred-dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is equal to the predictive probability (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pred-prob"
plural "false"
caps "false"
noprefix "false"

\end_inset

) obtained for the Bayesian Linear Regression problem (Section 2).
 Using the linear covaraince defined in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cov func f"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we obtain
\begin_inset Formula 
\begin{align}
K & =X^{T}\Sigma_{w}X\label{eq:K}\\
\boldsymbol{k}_{*} & =X^{T}\Sigma_{w}\boldsymbol{x}_{*}\label{eq:k_star}\\
\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*}) & =\boldsymbol{x}_{*}^{T}\Sigma_{w}\boldsymbol{x}_{*}\label{eq:k_star_star}
\end{align}

\end_inset

Substituting into the varaince expression (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:post-pred-var-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and using the matrix inverse lemma (see Appendix D) we obtain, 
\begin_inset Formula 
\begin{align*}
\mathrm{var}[f_{*}|{D}] & =\boldsymbol{x}_{*}^{T}\Sigma_{w}\boldsymbol{x}_{*}-\boldsymbol{x}_{*}^{T}\Sigma_{w}X(X^{T}\Sigma_{w}X+\sigma_{\varepsilon}^{2}I)^{-1}X^{T}\Sigma_{w}\boldsymbol{x}_{*}=\\
 & =\boldsymbol{x}_{*}^{T}(\Sigma_{w}-X(X^{T}\Sigma_{w}X+\sigma_{\varepsilon}^{2}I)^{-1}X^{T})\boldsymbol{x}_{*}=\\
 & =\boldsymbol{x}_{*}^{T}(\Sigma_{w}^{-1}+\sigma_{\varepsilon}^{-1}XX^{T})^{-1}\boldsymbol{x}_{*}
\end{align*}

\end_inset

which is equal to the varaince of the predictive probability in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pred-prob"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Similarly, substituting (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:K"
plural "false"
caps "false"
noprefix "false"

\end_inset

)-(
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:k_star_star"
plural "false"
caps "false"
noprefix "false"

\end_inset

) into the mean (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:post-pred-mean-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we obtain, 
\begin_inset Formula 
\begin{align*}
\mathcal{\mathbb{{E}}}[f_{*}|\mathcal{{D}}] & =\boldsymbol{k}_{*}^{T}(K+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{y}=\\
 & =\boldsymbol{x}_{*}^{T}\Sigma_{w}X(X^{T}\Sigma_{w}X+\sigma_{\varepsilon}^{2}I)^{-1}\boldsymbol{y}=\\
\end{align*}

\end_inset

Since 
\begin_inset Formula $\Sigma_{w}$
\end_inset

 is symmetric and postive-definite we can use a varaint of the matrix inverse
 lemma (Appendix D) to obtain 
\begin_inset Formula 
\[
\mathcal{\mathbb{{E}}}[f_{*}|\mathcal{{D}}]=\frac{1}{\sigma_{\varepsilon}^{2}}\boldsymbol{x}_{*}^{T}(\sigma_{\varepsilon}^{-2}XX^{T}-\Sigma_{w}^{-1})^{-1}X\boldsymbol{y}
\]

\end_inset

which is equal to the mean of the predictive probability in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pred-prob"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Section*
Appendix A
\end_layout

\begin_layout Standard
Using the Bayesian Linear Regression model, the process 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 is defined as
\begin_inset Formula 
\[
f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x},\quad\boldsymbol{w}\sim\mathcal{{N}}(0,\Sigma_{w}).
\]

\end_inset

We would like to show that 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 is Gaussian process.
 
\begin_inset Newline newline
\end_inset

Definition A.1: A Gaussian process is a collection if random variables, any
 finite number of which have a joint Gaussian distribution.
 
\begin_inset Newline newline
\end_inset

Lemma A.1: A collection of Gaussian random variables are jointly Gaussian
 and form a joint Gaussian distribution if any linear combination of the
 variables is a Gaussian random variable.
 
\begin_inset Newline newline
\end_inset

Denote 
\begin_inset Formula $f(x_{i}),i=1,..,n$
\end_inset

 the set of random variables corresponding to 
\begin_inset Formula $n$
\end_inset

 input point 
\begin_inset Formula $x_{i}$
\end_inset

.
 Further denote aith 
\begin_inset Formula $a_{i}\in\mathbb{{R}},i=1,..,n$
\end_inset

 a set of deterministic values.
 Thus, 
\begin_inset Formula 
\[
\sum_{i=1}^{n}a_{i}f(\boldsymbol{x}_{i})=\sum_{i=1}^{n}a_{i}\boldsymbol{w}^{T}\boldsymbol{x}_{i}=\sum_{i=1}\boldsymbol{w}^{T}a_{i}\boldsymbol{x}_{i}=\boldsymbol{w}^{T}\underbrace{\sum_{i=1}^{n}a_{i}\boldsymbol{x}_{i}}_{\boldsymbol{Y}}=\boldsymbol{w}^{T}\boldsymbol{Y}.
\]

\end_inset

Since 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 is Gaussian random vector,
\begin_inset Formula $\boldsymbol{w}^{T}\boldsymbol{Y}$
\end_inset

 is Gaussian variable (linear combination of Gaussian variables) and according
 to Lemma A.1 
\begin_inset Formula $f(\boldsymbol{x}_{1}),f(\boldsymbol{x}_{2}),...,f(\boldsymbol{x}_{n})$
\end_inset

 are jointly Gaussian.
 Hence, according to Definition A.1 
\begin_inset Formula $f(\boldsymbol{x})$
\end_inset

 is Gaussian process with mean and covariance function satisfying
\begin_inset Formula 
\begin{align*}
E[f(\boldsymbol{x})] & =0\\
E[f(\boldsymbol{x})f(\boldsymbol{x}')] & =\boldsymbol{x}^{T}\Sigma_{w}\boldsymbol{x'}
\end{align*}

\end_inset


\end_layout

\begin_layout Section*
Appendix B 
\end_layout

\begin_layout Standard
From Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:posterior"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we obtain 
\begin_inset Formula 
\begin{align*}
\log p(\boldsymbol{f}|\mathcal{{D}}) & =C-\frac{1}{2\sigma_{\varepsilon}^{2}}(\boldsymbol{y}-\boldsymbol{f})^{T}(\boldsymbol{y}-\boldsymbol{f})-\frac{1}{2}\boldsymbol{f}^{T}K^{-1}\boldsymbol{f}=\\
 & =C-\frac{1}{2\sigma_{\varepsilon}^{2}}(\boldsymbol{y}^{T}\boldsymbol{y}-2\boldsymbol{y}^{T}\boldsymbol{f+f}^{T}\boldsymbol{f})-\frac{1}{2}\boldsymbol{f}^{T}K^{-1}\boldsymbol{f=}\\
 & =C+\frac{1}{\sigma_{\varepsilon}^{2}}\boldsymbol{y}^{T}\boldsymbol{f-}\frac{1}{2}\boldsymbol{f}^{T}\left[\sigma_{\varepsilon}^{-2}I+K^{-1}\right]\boldsymbol{f}=\\
 & =C-\frac{1}{2}\{\boldsymbol{f}^{T}\left[\sigma_{\varepsilon}^{-2}I+K^{-1}\right]\boldsymbol{f}-2\frac{1}{\sigma_{\varepsilon}^{2}}\boldsymbol{y}^{T}\boldsymbol{f}\}
\end{align*}

\end_inset

Denotes 
\begin_inset Formula $A=\left[\sigma_{\varepsilon}^{-2}I+K^{-1}\right]$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{b}=\frac{1}{\sigma_{\varepsilon}^{2}}\boldsymbol{y}$
\end_inset

.
 Completing the square yiedls
\begin_inset Formula 
\begin{align*}
\log p(\boldsymbol{f}|\mathcal{{D}}) & =C-\frac{1}{2}(\boldsymbol{f}^{T}A\boldsymbol{f}-2\boldsymbol{b}^{T}\boldsymbol{f})\\
 & =C-\frac{1}{2}\left[(\boldsymbol{f}-A^{-1}\boldsymbol{b})^{T}A(\boldsymbol{f}-A^{-1}\boldsymbol{b})+\boldsymbol{b}^{T}A^{-1}\boldsymbol{b}\right]
\end{align*}

\end_inset

and hence, 
\begin_inset Formula 
\begin{align*}
\mathcal{\mathbb{{E}}}[\boldsymbol{f}|\mathcal{{D}}] & =K(\sigma_{\varepsilon}^{2}I+K)^{-1}\boldsymbol{y}\\
\mathrm{cov}[\boldsymbol{f}|\mathcal{{D}}] & =\sigma_{\varepsilon}^{2}(\sigma_{\varepsilon}^{2}I+K)^{-1}K
\end{align*}

\end_inset


\end_layout

\begin_layout Section*
Appendix C
\end_layout

\begin_layout Standard
We would like to compute the posterior predictive distribution defined in
 Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:posterior predictive"
plural "false"
caps "false"
noprefix "false"

\end_inset

) (reproducted here for clarity)
\begin_inset Formula 
\[
p(f_{*}|\mathcal{{D}})=\int p(f_{*}|\boldsymbol{f})p(\boldsymbol{f}|\mathcal{{D}})d\boldsymbol{f}
\]

\end_inset

where the distributions 
\begin_inset Formula $p(f_{*}|\boldsymbol{f})$
\end_inset

 and 
\begin_inset Formula $p(\boldsymbol{f}|\mathcal{{D}})$
\end_inset

 are Gaussian with 
\begin_inset Formula 
\begin{align*}
p(\boldsymbol{f}|\mathcal{{D}}) & =\mathcal{{N}}(\boldsymbol{\mu}_{p},\boldsymbol{\Sigma}_{p})\\
p(f_{*}|\boldsymbol{f}) & =\mathcal{{N}}(\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{f},\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})-\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{k}_{*})
\end{align*}

\end_inset

We use the follwoing property (Bishop et al., Section 2.3.3) of Gaussian distributi
on.
\begin_inset Newline newline
\end_inset

Property B.1: Consider a marginal Gaussian 
\begin_inset Formula $p(\boldsymbol{x})$
\end_inset

 and a Gaussian conditional distribution 
\begin_inset Formula $p(\boldsymbol{y}|\boldsymbol{x})$
\end_inset

 in which 
\begin_inset Formula $p(\boldsymbol{y}|\boldsymbol{x})$
\end_inset

 has a mean that is a linear function of 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

, and a covariance which is independent of 
\begin_inset Formula $\boldsymbol{x}$
\end_inset


\begin_inset Formula 
\begin{align*}
p(\boldsymbol{x}) & =\mathcal{{N}}(\boldsymbol{x}|\boldsymbol{\mu},\Lambda^{-1}),\\
p(\boldsymbol{y}|\boldsymbol{x}) & =\mathcal{{N}}(\boldsymbol{y}|A\boldsymbol{x}+\boldsymbol{b},L^{-1}).
\end{align*}

\end_inset

Here 
\begin_inset Formula $\boldsymbol{\mu},A,\boldsymbol{b}$
\end_inset

 are parameters governing the means, and 
\begin_inset Formula $\Lambda,L$
\end_inset

 are the precision matrices (inverse coavariance matrices).
 If 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 has dimensionality 
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

 has dimensionality 
\begin_inset Formula $D$
\end_inset

 then the matrix 
\begin_inset Formula $A$
\end_inset

 has size 
\begin_inset Formula $D\times M$
\end_inset

.
 The marginal distribution 
\begin_inset Formula $p(\boldsymbol{y})$
\end_inset

 is Gaussian with mean and covaraince given by
\begin_inset Formula 
\begin{align*}
\mathcal{\mathbb{{E}}}[\boldsymbol{y}] & =A\boldsymbol{\mu+}b,\\
\mathrm{cov}[\boldsymbol{y}] & =L^{-1}+A\Lambda^{-1}A^{T}
\end{align*}

\end_inset

using property B.1 we obtain
\begin_inset Formula 
\begin{align*}
\mathcal{\mathbb{{E}}}[f_{*}] & =\boldsymbol{k}_{*}^{T}K^{-1}\boldsymbol{\mu}_{p}\\
\mathrm{var}[f_{*}] & =\mathcal{{K}}(\boldsymbol{x}_{*},\boldsymbol{x}_{*})+\boldsymbol{k}_{*}^{T}(K^{-1}\Sigma_{p}K^{-1}-K^{-1})\boldsymbol{k}_{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Section*
Appendix D 
\end_layout

\begin_layout Standard
The 
\shape italic
matrix inverse lemma
\shape default
, also known as the Woodbury, Sherman & Morrison formula states that 
\begin_inset Formula 
\begin{equation}
(Z+UWV^{T})^{-1}=Z^{-1}-Z^{-1}U(W^{-1}+V^{T}Z^{-1}U)^{-1}V^{T}Z^{-1},
\end{equation}

\end_inset

assuming the relevant inverses all exist.
 Here 
\begin_inset Formula $Z$
\end_inset

 is 
\begin_inset Formula $n\times n$
\end_inset

, 
\begin_inset Formula $W$
\end_inset

 is 
\begin_inset Formula $m\times m$
\end_inset

 and 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

 are both of size 
\begin_inset Formula $n\times m$
\end_inset

.
 
\begin_inset Newline newline
\end_inset

For positive-definite matrices 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 we obtain the following varaint of the matrix inverse lemma
\begin_inset Formula 
\[
(P^{-1}+B^{T}R^{-1}B)^{-1}B^{T}R^{-1}=PB^{T}(BPB^{T}+R)^{-1}.
\]

\end_inset

We note that if 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 are also symmetric, 
\begin_inset Formula $(P^{-1}+B^{T}R^{-1}B)^{-1}$
\end_inset

 is symmetric function (symmetry is closed under summation and inverse)
 and hence
\begin_inset Formula 
\[
R^{-1}B(P^{-1}+B^{T}R^{-1}B)^{-1}=(BPB^{T}+R)^{-1}BP^{T}
\]

\end_inset


\end_layout

\end_body
\end_document
